{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emUrqXUTTkGe",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install pylance\n",
        "pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import lightning\n",
        "\n",
        "import lance\n",
        "import pyarrow as pa\n",
        "\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "VS8cYBmTTo3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset and Its Creation"
      ],
      "metadata": {
        "id": "RMPFTbK9QoG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/intern_screening_dataset.csv')"
      ],
      "metadata": {
        "id": "_Deh6loj8-zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text, pattern=r'https?://\\S+'):\n",
        "    #Removes URLs from text using a provided regular expression pattern.\n",
        "    return re.sub(pattern, \"\", text)\n",
        "\n",
        "\n",
        "def remove_extra_spaces(text):\n",
        "    #Removes excessive white spaces from text.\n",
        "    return \" \".join(text.split())  # Splits text on spaces and rejoins with single spaces\n",
        "\n",
        "#html tags did not exist in the data while punctuation is not removed because it does play an important role in the answer"
      ],
      "metadata": {
        "id": "-WyqC5xpFasv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset.dropna() #Removes missing values from the dataset\n",
        "data[\"answer\"] = data[\"answer\"].apply(remove_urls)\n",
        "data[\"answer\"] = data[\"answer\"].apply(remove_extra_spaces)"
      ],
      "metadata": {
        "id": "wA2x840aFbNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine question and answer columns\n",
        "data['text'] = data['question'] + \" \" + data['answer']\n",
        "\n",
        "df, testdata = train_test_split(data, test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "kZVtXMwOFfqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of sentences (adjust for desired training size)\n",
        "total_rows = 1000\n",
        "\n",
        "# Select a subset of rows (optional)\n",
        "data = df.iloc[:total_rows]  # Use .iloc for integer-based indexing\n",
        "\n",
        "# Initialize an empty list for all tokens\n",
        "all_tokens = []\n",
        "\n",
        "for row in data['text']:\n",
        "  encoded = tokenizer(row)['input_ids']\n",
        "  all_tokens.extend(encoded)\n",
        "\n",
        "# Create a PyArrow Table from the tokenized data\n",
        "pa_table = pa.Table.from_arrays([all_tokens], names=['value'])\n",
        "\n",
        "# Save the PyArrow Table using lance (assuming you have it installed)\n",
        "lance.write_dataset(pa_table, \"medical_dataset.lance\", {'model': 'create'})\n",
        "\n",
        "print(f\"Total tokens in tokenized dataset: {len(all_tokens):,.0f}\")"
      ],
      "metadata": {
        "id": "vC77_3rtTrwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model and Training¶\n"
      ],
      "metadata": {
        "id": "Xe-YTaUhQ6vs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuKOF5g5-KvG"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    vocab_size = 50304\n",
        "    lr = 3e-4\n",
        "    wd = 1e-6\n",
        "    n_embed = 256\n",
        "    num_blocks = 12\n",
        "    num_heads = 12\n",
        "    head_size = n_embed // num_heads\n",
        "    context_len = 224\n",
        "    attn_dropout_val = 0.2\n",
        "    mha_dropout_val = 0.2\n",
        "    ffn_dropout_val = 0.2\n",
        "    n_epochs = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention - CausalAttentionHead and MultiHeadedAttention"
      ],
      "metadata": {
        "id": "3PwAZUCnQ3Rz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXpSzhmA-Mz4"
      },
      "outputs": [],
      "source": [
        "class CausalAttentionHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(CausalAttentionHead, self).__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # QKV layers\n",
        "        self.query = nn.Linear(config.n_embed, config.head_size, bias=False)\n",
        "        self.key = nn.Linear(config.n_embed, config.head_size, bias=False)\n",
        "        self.value = nn.Linear(config.n_embed, config.head_size, bias=False)\n",
        "        self.attn_drop = nn.Dropout(config.attn_dropout_val)\n",
        "\n",
        "        # Mask for ensuring causality during training\n",
        "        self.register_buffer('mask', torch.tril(torch.ones(config.context_len, config.context_len)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shape of x: [bs, context_len, embed_dim]\n",
        "        bs, context_len, embed_dim = x.shape\n",
        "        q, k, v = self.query(x), self.key(x), self.value(x)\n",
        "\n",
        "        # Get the attention weights\n",
        "        attn_filter = torch.divide(torch.bmm(q, k.transpose(1, 2)), self.config.head_size)\n",
        "        attn_filter = attn_filter.masked_fill(self.mask[:context_len, :context_len]==0, float('-inf'))\n",
        "        attn_weights = F.softmax(attn_filter, dim=-1)\n",
        "        attn_weights = self.attn_drop(attn_weights)\n",
        "\n",
        "        # Now we do weighted aggregation of values to get the output of attention\n",
        "        # attn_weights [bs, c, c] x V [bs, c, h] = output [bs, c, head_size]\n",
        "        output = torch.bmm(attn_weights, v)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f--F6ut-n3M"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Turn all the AttentionHeads into a ModuleList\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CausalAttentionHead(config) for _ in range(config.num_heads)]\n",
        "        )\n",
        "\n",
        "        # Projection and Dropout that projects mha_output it back to n_embed dim\n",
        "        self.proj = nn.Linear(config.num_heads*config.head_size, config.n_embed)\n",
        "        self.mha_drop = nn.Dropout(config.mha_dropout_val)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Concatenate all the attention head outputs together\n",
        "        mha_output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        return self.mha_drop(self.proj(mha_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FeedForward Network"
      ],
      "metadata": {
        "id": "7G6TWuVwRCjq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwRgLmcz-p-A"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNet(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(FeedForwardNet, self).__init__()\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(config.n_embed, config.n_embed*4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(config.n_embed*4, config.n_embed),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ffn(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Single Block of the GPT model¶"
      ],
      "metadata": {
        "id": "n_Q5ednpRGwk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKlqKZ07-qkE"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        # Architecture of one block of GPT\n",
        "        self.mha = MultiHeadedAttention(config)\n",
        "        self.ln1 = nn.LayerNorm(config.n_embed)\n",
        "        self.ffn = FeedForwardNet(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x + self.mha(x))\n",
        "        x = self.ln2(x + self.ffn(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entire GPT model, end-to-end¶\n"
      ],
      "metadata": {
        "id": "LnWyXjpBRKCU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyi8UZQK-sU0"
      },
      "outputs": [],
      "source": [
        "class GPT(lightning.LightningModule):\n",
        "    def __init__(self, config):\n",
        "        super(GPT, self).__init__()\n",
        "        self.config = config\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Define token and positional embeddings\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.n_embed)\n",
        "        self.positional_embedding = nn.Embedding(config.context_len, config.n_embed)\n",
        "\n",
        "        # Define the blocks\n",
        "        self.backbone = nn.Sequential(*[Block(config) for _ in range(config.num_blocks)])\n",
        "\n",
        "        # Define the LM head\n",
        "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply token embeddings through the data (B, C) -> (B, C, V)\n",
        "        tok_emb = self.token_embedding(x)\n",
        "\n",
        "        # Get positional embeddings using torch.arange\n",
        "        pos_emb = self.positional_embedding(torch.arange(x.shape[1], device=self.device))\n",
        "\n",
        "        # Add both embeddings\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        # Pass the input data through all blocks\n",
        "        x = self.backbone(x)\n",
        "\n",
        "        # Pass it through the lm head\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "    def get_loss(self, predictions, target):\n",
        "        B, C, V = predictions.shape\n",
        "        predictions = predictions.view(B*C, V)\n",
        "        target = target.view(B*C)\n",
        "        loss = F.cross_entropy(predictions, target)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        text, target = batch\n",
        "        text = text.long()\n",
        "        target = target.long()\n",
        "        logits = self(text)\n",
        "        loss = self.get_loss(logits, target)\n",
        "\n",
        "        self.log('loss', loss.item(), prog_bar=True)\n",
        "\n",
        "        logs = {'loss': loss}\n",
        "        return {'log': logs, 'loss': loss}\n",
        "\n",
        "    def training_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['log']['loss'] for x in outputs]).mean()\n",
        "\n",
        "        logs = {'loss': avg_loss}\n",
        "\n",
        "        print(f\"val_loss: {avg_loss}\")\n",
        "        return {'log': logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.AdamW(self.parameters(), lr=self.config.lr, weight_decay=self.config.wd)\n",
        "        return [opt], []\n",
        "\n",
        "def generate(model, prompt, max_tokens, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Generates text based on the provided prompt.\n",
        "    Model determinism can be changed with temperature\n",
        "    (range: [0, 1], higher means more unstable but creative predictions)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    for _ in range(max_tokens):\n",
        "        prompt = prompt[:, :config.context_len]\n",
        "        logits = model(prompt)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        logit_probs = nn.functional.softmax(logits, dim=-1)\n",
        "        next_prompt = torch.multinomial(logit_probs, num_samples=1)\n",
        "        prompt = torch.cat((prompt, next_prompt), dim=1)\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPTDataset for efficient and fast data loading using Lance"
      ],
      "metadata": {
        "id": "SVDbeLi9RNd5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxpp7ebA-wLC"
      },
      "outputs": [],
      "source": [
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, dataset_path, context_len):\n",
        "        # Load the lance dataset from the saved path\n",
        "        self.ds = lance.dataset(dataset_path)\n",
        "        self.context_len = context_len\n",
        "        # Doing this so the sampler never asks for an index at the end of text\n",
        "        self.length = self.ds.count_rows() - context_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def from_idxs(self, idxs):\n",
        "        \"\"\"\n",
        "        Little Utility function to get the data from lance\n",
        "        \"\"\"\n",
        "        data = self.ds.take(idxs).to_pylist()\n",
        "        data = torch.tensor(list(map(lambda x: x['value'], data)))\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Generate a list of indices starting from the current idx to idx+context_len+1\n",
        "        Use the from_idxs function to get data in said indexes and then divide it into features (x) and target (y)\n",
        "        \"\"\"\n",
        "        current_window_idxs = np.arange(idx, idx+self.context_len+1)\n",
        "        data = self.from_idxs(current_window_idxs)\n",
        "        x = data[0:self.context_len]\n",
        "        y = data[1:self.context_len+1] # +1 because our target is the sentence is 1 step ahead of input text\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Traning Sequence"
      ],
      "metadata": {
        "id": "kxr9Cp2eRVhZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vjUT4r1_YBL"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Path of the encoded lance dataset\n",
        "    dataset_path = \"medical_dataset.lance\"\n",
        "\n",
        "    # Init config\n",
        "    config = Config()\n",
        "\n",
        "    # Init model\n",
        "    gpt = GPT(config)\n",
        "\n",
        "    # Init the dataset\n",
        "    dataset = GPTDataset(dataset_path, config.context_len)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    # Init the trainer\n",
        "    trainer = lightning.Trainer(accelerator='auto', max_epochs=config.n_epochs)\n",
        "\n",
        "    # Fit on the data\n",
        "    trainer.fit(gpt, loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation using the Rouge(W) F1 score"
      ],
      "metadata": {
        "id": "BHdk7__4RYug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "def evaluate_rouge(predictions, references):\n",
        "\n",
        "  rouge = Rouge()\n",
        "  scores = rouge.get_scores(predictions, references)\n",
        "\n",
        "  score = scores[0][\"rouge-l\"][\"f\"]\n",
        "\n",
        "  return score"
      ],
      "metadata": {
        "id": "jymdaNZs_NqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test (prompt):\n",
        "  tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "  gpt = gpt.to('cuda')\n",
        "  prompt = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
        "  generated_text = generate(gpt, prompt, max_tokens=config.context_len, temperature=0.7)\n",
        "  generated_text = tokenizer.decode(generated_text.tolist()[0])\n",
        "  return generated_text"
      ],
      "metadata": {
        "id": "WXZ41syM-GwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdata[\"prediction\"] = test(testdata[\"question\"])\n",
        "\n",
        "testdata[\"rouge_score\"] = evaluate_rouge(testdata[\"prediction\"], testdata[\"answer\"])\n",
        "\n",
        "rouge_score = testdata[\"rouge_score\"].mean()"
      ],
      "metadata": {
        "id": "k10ZXf07_g9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Prompts"
      ],
      "metadata": {
        "id": "8Dq-TRVrRg_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Is Rubinstein-Taybi syndrome inherited ?\" # Change the prompt to whatever you want\n",
        "print(test(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t9C0CZY-u1X",
        "outputId": "fe256140-ba83-405d-f967-a605dccf81a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rubinstein-Taybi syndrome typically arises due to de novo mutations, meaning they occur spontaneously rather than being inherited from parents. However, in a small percentage of cases, it can be inherited in an autosomal dominant pattern, where a mutated gene from one parent is sufficient to cause the condition. Therefore, while most cases are not inherited, there is a genetic component in some instances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What are the treatments for Danon disease ?\"\n",
        "print(test(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj7Fz7Up-wQ8",
        "outputId": "02fd7b04-a453-4c05-a6d1-9e01769668a6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treatment options for Danon disease encompass medication to address symptoms such as heart failure and arrhythmias, implantable defibrillators for severe cases, cardiac transplantation for advanced heart issues, physical therapy to maintain muscle strength, and genetic counseling for understanding inheritance patterns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Is oculopharyngeal muscular dystrophy inherited ?\"\n",
        "print(test(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm3LmhRy_Icx",
        "outputId": "ba2c2145-f652-45cf-943e-31443fcd14e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, oculopharyngeal muscular dystrophy (OPMD) is inherited in an autosomal dominant pattern. This means that a person only needs to inherit one copy of the mutated gene from one parent to develop the disorder. Therefore, individuals with a parent affected by OPMD have a 50% chance of inheriting the mutated gene and developing the condition themselves.\n"
          ]
        }
      ]
    }
  ]
}